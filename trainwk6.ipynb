{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainwk6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGARM32qbQNz"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------------------------------\n",
        "#  Copyright (c) Leo Hanisch. All rights reserved.\n",
        "#  Licensed under the BSD 3-Clause License. See LICENSE.txt in the project root for license information.\n",
        "# ------------------------------------------------------------------------------------------------------\n",
        "\n",
        "from os import path\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import Tensor, save, load\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "from linear_regression_model import LinearRegressionModel\n",
        "\n",
        "# pylint: disable=too-many-statements,too-many-locals\n",
        "\n",
        "def train():\n",
        "    # Load data\n",
        "    data_set_path = path.join(path.abspath(path.dirname(__file__)), '../resources/eclipse-data-set.csv')\n",
        "    data = np.genfromtxt(data_set_path, delimiter=';', skip_header=1, usecols=[1, 2, 3, 4, 5, 6])\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    n_rows = data.shape[0]\n",
        "    train_rows = int(n_rows*0.8)\n",
        "    test_rows = int((n_rows - train_rows)/2)\n",
        "\n",
        "    # x_input = Variable(Tensor(data[:, 3].reshape((-1, 1))))  # third column is linear entropy\n",
        "    # x_input = Variable(Tensor([[1.0], [2.0], [3.0]]))\n",
        "    # y_truth = Variable(Tensor([[2.0], [4.0], [6.0]]))\n",
        "\n",
        "    epochs = 2001\n",
        "    criterion = MSELoss()\n",
        "\n",
        "    entropy_map = {\n",
        "        1: 'full_not_decayed',\n",
        "        2: 'weighted_not_decayed',\n",
        "        3: 'full_linear_decayed',\n",
        "        4: 'full_log_decayed',\n",
        "        5: 'full_exp_decayed'\n",
        "    }\n",
        "\n",
        "    learing_rate_map = {\n",
        "        1: (0.0000005, 0.000001),\n",
        "        2: (0.0001, 0.005),\n",
        "        3: (0.0001, 0.005),\n",
        "        4: (0.0001, 0.005),\n",
        "        5: (0.0001, 0.005)\n",
        "    }\n",
        "    for entropy_col, entropy_type in entropy_map.items():\n",
        "        y_train = Variable(Tensor(data[:train_rows, 0].reshape((-1, 1))))  # first column is number of bugs\n",
        "        x_train = Variable(Tensor(data[:train_rows, entropy_col].reshape((-1, 1))))  # third column is weighted entropy\n",
        "        y_val = Variable(Tensor(data[train_rows+1:train_rows+test_rows, 0].reshape((-1, 1))))  # first column is number of bugs\n",
        "        x_val = Variable(Tensor(data[train_rows+1:train_rows+test_rows, entropy_col].reshape((-1, 1))))  # third column is weighted entropy\n",
        "        y_test = Variable(Tensor(data[train_rows+test_rows+1:, 0].reshape((-1, 1))))  # first column is number of bugs\n",
        "        x_test = Variable(Tensor(data[train_rows+test_rows+1:, entropy_col].reshape((-1, 1))))  # third column is weighted entropy\n",
        "\n",
        "        model_file_name = entropy_type + '_hcm'\n",
        "        model_dir = path.normpath(path.join(path.abspath(path.dirname(__file__)), '../resources/models'))\n",
        "        model_file_path = path.join(model_dir, model_file_name + '.pt')\n",
        "\n",
        "        learing_rates = np.linspace(*learing_rate_map[entropy_col])\n",
        "\n",
        "        # Try to load model\n",
        "        old_model = LinearRegressionModel(1, 1)\n",
        "        try:\n",
        "            old_model.load_state_dict(load(model_file_path))\n",
        "        except FileNotFoundError:\n",
        "            print('File=\"{}\" was not found. Create new model.'.format(model_file_path))\n",
        "        y_test_pred_old = old_model(x_test)\n",
        "        y_test_loss_old = criterion(y_test_pred_old, y_test)\n",
        "\n",
        "        for learing_rate in learing_rates:\n",
        "            train_loss_list = []\n",
        "            val_loss_list = []\n",
        "            test_loss_list = []\n",
        "\n",
        "            print('Train for entropy type=\"{0}\" and learning rate=\"{1}\"'.format(entropy_type, learing_rate))\n",
        "            model = LinearRegressionModel(1, 1)\n",
        "\n",
        "            optimizer = SGD(model.parameters(), lr=learing_rate)\n",
        "            for epoch in range(epochs):\n",
        "\n",
        "                # Forward pass: Compute predicted y by passing\n",
        "                # x to the model\n",
        "                y_predicted = model(x_train)\n",
        "\n",
        "                # Compute and print loss\n",
        "                train_loss = criterion(y_predicted, y_train)\n",
        "                train_loss_list.append(float(train_loss.data))\n",
        "\n",
        "                # Val loss\n",
        "                y_val_pred = model(x_val)\n",
        "                val_loss = criterion(y_val_pred, y_val)\n",
        "                val_loss_list.append(float(val_loss.data))\n",
        "\n",
        "                # Zero\n",
        "                optimizer.zero_grad()\n",
        "                train_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                y_test_pred = model(x_test)\n",
        "                test_loss = criterion(y_test_pred, y_test)\n",
        "                test_loss_list.append(float(test_loss.data))\n",
        "\n",
        "                if epoch % 50 == 0:\n",
        "                    print('epoch {0}, loss {1}'.format(epoch, train_loss.data))\n",
        "                    if test_loss.data < y_test_loss_old.data:\n",
        "                        print('Save model with prediction error {}.'.format(test_loss))\n",
        "                        save(model.state_dict(), model_file_path)\n",
        "                        meta_data = {\n",
        "                            'history_complexity_metric': entropy_type,\n",
        "                            'learning_rate': learing_rate,\n",
        "                            'val_loss': val_loss_list,\n",
        "                            'train_loss': train_loss_list,\n",
        "                            'test_loss': test_loss_list\n",
        "                        }\n",
        "\n",
        "                        with open(path.join(model_dir, model_file_name + '_meta.json'), 'w') as write_file:\n",
        "                            json.dump(meta_data, write_file, indent=4)\n",
        "\n",
        "                        y_test_loss_old = test_loss\n",
        "\n",
        "    # Compute and print loss\n",
        "    test_loss = criterion(y_test_pred, y_test)\n",
        "    print('Test loss {0}'.format(test_loss.data))\n",
        "\n",
        "    train_plt, = plt.plot(range(epochs), train_loss_list, label='Train Loss')\n",
        "    val_plt, = plt.plot(range(epochs), val_loss_list, label='Validation Loss')\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(handles=[train_plt, val_plt])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ]
    }
  ]
}